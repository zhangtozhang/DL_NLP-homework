import os
import re
import gensim.models as w2v
import jieba

def load_stopwords(file_path):
    """
    从指定文件路径加载停用词
    :param file_path: 停用词文件的路径
    :return: 停用词列表
    """
    with open(file_path, 'r', encoding='UTF-8') as f:
        return [line.strip() for line in f.readlines()]

def content_deal(content, stopwords):
    """
    处理文本内容，去除广告、非中文字符和停用词
    :param content: 待处理的文本内容
    :param stopwords: 停用词列表
    :return: 处理后的文本内容
    """
    # 定义要去除的广告内容
    ad = [
        '本书来自www.cr173.com免费txt小说下载站\n更多更新免费电子书请关注www.cr173.com',
        '----〖新语丝电子文库(www.xys.org)〗',
        '新语丝电子文库'
    ]
    for a in ad:
        content = content.replace(a, '')
    # 去除非中文字符
    content = re.sub(r'[^\u4e00-\u9fa5]', '', content)
    words = [word for word in words if word not in stopwords]
    return words

def load_data(data_dir, stopwords):
    """
    从指定目录加载数据并进行处理
    :param data_dir: 数据目录的路径
    :param stopwords: 停用词列表
    :return: 处理后的语料库，以及每本书的行数
    """
    corpus = []
    book_lines = []
    total_lines = 0
    for root, dirs, files in os.walk(data_dir):
        for file in files:
            file_path = os.path.join(root, file)
            book_corpus = []
            with open(file_path, 'r', encoding='ANSI') as f:
                for line in f:
                    # 处理每一行文本
                    line = line.strip()
                    if line:
                        # 去除广告和非中文字符
                        ad = [
                            '本书来自www.cr173.com免费txt小说下载站\n更多更新免费电子书请关注www.cr173.com',
                            '----〖新语丝电子文库(www.xys.org)〗',
                            '新语丝电子文库'
                        ]
                        for a in ad:
                            line = line.replace(a, '')
                        line = re.sub(r'[^\u4e00-\u9fa5]', '', line)
                        # 分词
                        words = jieba.lcut(line)
                        # 去除停用词
                        words = [word for word in words if word not in stopwords]
                        if words:
                            book_corpus.append(words)
            num_lines = len(book_corpus)
            book_lines.append(num_lines)
            total_lines += num_lines
            corpus.extend(book_corpus)

    # for i, lines in enumerate(book_lines):
    #     print(f"第 {i + 1} 本书的分词行数: {lines}")
    # print(f"总行数: {total_lines}")

    return corpus

# 加载停用词
stopwords_path = r'E:\pc_zhang\2025_spring_term\class_study\DL_NLP\homework3\stopwords.txt'
stopwords = load_stopwords(stopwords_path)

# 加载数据
data_dir = r'E:\pc_zhang\2025_spring_term\class_study\DL_NLP\homework3\data_jinyong'
corpus = load_data(data_dir, stopwords)

# for line in corpus[1:10]:
#     print(line)

# print(len(corpus))
#CBOW模型
model = w2v.Word2Vec(sentences=corpus, vector_size=200, window=5, min_count=5, workers=4, sg=0)
model.save("CBOW.model")

def find_relation(a, b, c):
    d, _ = model.wv.most_similar(positive=[c, b], negative=[a])[0]
    print (c,d)


print(f'令狐冲人物关系分析：{model.wv.most_similar("令狐冲", topn=10)}')
print(f'令狐冲——岳灵珊 关系相关度：{model.wv.similarity("令狐冲", "岳灵珊")}')
print(f'令狐冲——任盈盈 关系相关度：{model.wv.similarity("令狐冲", "盈盈")}')
print(f'令狐冲——仪琳 关系相关度：{model.wv.similarity("令狐冲", "仪琳")}')
print(f'令狐冲——岳不群 关系相关度：{model.wv.similarity("令狐冲", "岳不群")}')
print(f'令狐冲——林平之 关系相关度：{model.wv.similarity("令狐冲", "林平之")}')
print(f'令狐冲——风清扬 关系相关度：{model.wv.similarity("令狐冲", "风清扬")}')
print(f'令狐冲——劳德诺 关系相关度：{model.wv.similarity("令狐冲", "劳德诺")}')
print(f'林平之——岳灵珊 关系相关度：{model.wv.similarity("林平之", "岳灵珊")}')

find_relation('令狐冲', '岳灵珊','段誉')
find_relation('令狐冲', '岳不群','段誉')
find_relation('令狐冲', '林平之','段誉')
find_relation('令狐冲', '岳灵珊','段誉')
find_relation('令狐冲', '盈盈','段誉')
find_relation('令狐冲', '仪琳','段誉')

#skip-gram模型
model = w2v.Word2Vec(sentences=corpus, vector_size=200, window=5, min_count=5, workers=4, sg=1)
model.save("skip_gram.model")

print(f'令狐冲人物关系分析：{model.wv.most_similar("令狐冲", topn=10)}')
print(f'令狐冲——岳灵珊 关系相关度：{model.wv.similarity("令狐冲", "岳灵珊")}')
print(f'令狐冲——任盈盈 关系相关度：{model.wv.similarity("令狐冲", "盈盈")}')
print(f'令狐冲——仪琳 关系相关度：{model.wv.similarity("令狐冲", "仪琳")}')
print(f'令狐冲——岳不群 关系相关度：{model.wv.similarity("令狐冲", "岳不群")}')
print(f'令狐冲——林平之 关系相关度：{model.wv.similarity("令狐冲", "林平之")}')
print(f'令狐冲——风清扬 关系相关度：{model.wv.similarity("令狐冲", "风清扬")}')
print(f'令狐冲——劳德诺 关系相关度：{model.wv.similarity("令狐冲", "劳德诺")}')
print(f'林平之——岳灵珊 关系相关度：{model.wv.similarity("林平之", "岳灵珊")}')

find_relation('令狐冲', '岳灵珊','段誉')
find_relation('令狐冲', '岳不群','段誉')
find_relation('令狐冲', '林平之','段誉')
find_relation('令狐冲', '岳灵珊','段誉')
find_relation('令狐冲', '盈盈','段誉')
find_relation('令狐冲', '仪琳','段誉')
