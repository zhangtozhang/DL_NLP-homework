import tensorflow as tf
import numpy as np

# 读取文本数据
path = 'E:/pc_zhang/2025_spring_term/class_study/DL_NLP/homework4/data_jinyong/越女剑.txt'
# text = open(path).read()
with open(path,encoding='ANSI') as f:
    text = f.read()
# print('Corpus length:', len(text))
# ad = [  '本书来自www.cr173.com免费txt小说下载站\n更多更新免费电子书请关注www.cr173.com',
#         '----〖新语丝电子文库(www.xys.org)〗','新语丝电子文库','本书','免费小说','下载站',
#         '更新', '免费', '电子书', '请关注','小说', ',','','。'
#                         ]
# for a in ad:
#     text = text.replace(a, '')

# 数据预处理
# 创建字符到索引的映射
chars = sorted(set(text))
char_to_idx = {char: idx for idx, char in enumerate(chars)}
idx_to_char = {idx: char for idx, char in enumerate(chars)}
vocab_size = len(chars)

# 将文本转换为数字序列
text_as_int = np.array([char_to_idx[char] for char in text])

# 创建训练样本
seq_length = 100  # 每个输入序列的长度
examples_per_epoch = len(text) // seq_length

# 创建训练数据集
char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)
sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)

def split_input_target(sequence):
    input_text = sequence[:-1]
    target_text = sequence[1:]
    return input_text, target_text

dataset = sequences.map(split_input_target)

# 设置批量和缓冲区大小
BATCH_SIZE = 64
BUFFER_SIZE = 10000

dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

class Transformer(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, num_heads, depth, seq_length):
        super(Transformer, self).__init__()
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.transformer_block = [
            tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)
            for _ in range(depth)
        ]
        self.dense_layer = tf.keras.layers.Dense(vocab_size)

    def call(self, x):
        x = self.embedding(x)
        for block in self.transformer_block:
            x = block(x, x)
        return self.dense_layer(x)

# 超参数定义
EMBEDDING_DIM = 256
NUM_HEADS = 8
DEPTH = 4

# 创建模型
model = Transformer(vocab_size, EMBEDDING_DIM, NUM_HEADS, DEPTH, seq_length)

# 定义损失函数和优化器
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam()

model.compile(loss=loss, optimizer=optimizer)

# 训练模型
EPOCHS = 10
model.fit(dataset, epochs=EPOCHS)

def generate_text(start_string, num_generate, temperature):
    input_eval = [char_to_idx[s] for s in start_string]
    input_eval = tf.expand_dims(input_eval, 0)

    text_generated = []

    # model.reset_states()
    for i in range(num_generate):
        predictions = model(input_eval)
        predictions = tf.squeeze(predictions, 0)

        # 使用温度进行调整
        predictions = predictions / temperature
        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()

        # 用生成的字符作为下一个输入
        input_eval = tf.expand_dims(np.append(input_eval.numpy()[0][1:], predicted_id), 0)

        text_generated.append(idx_to_char[predicted_id])

    return start_string + ''.join(text_generated)

# 使用不同的温度生成文本
start_string = '越王勾践道：师兄，'
length = 400

# generated_text = generate_text(start_string, length, temperature=0.5)
# print(generated_text)
for temperature in [0.2, 0.5, 1.0, 1.2]:
    print(f"Temperature: {temperature}")
    generated_text = generate_text(start_string, length, temperature)
    print(generated_text)

